{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer for Text Generation\n",
        "\n",
        "Transformer models are advanced neural networks designed to handle sequential data without using recurrence. Unlike RNN, LSTM, and GRU, transformers rely on a self-attention mechanism to capture relationships between all tokens in a sequence simultaneously. This allows transformers to model long-term dependencies more effectively and process data in parallel. In text generation, transformers learn contextual relationships between words or characters and generate more coherent and meaningful text compared to recurrent models.\n"
      ],
      "metadata": {
        "id": "6jq1o7YMsfnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# TRANSFORMER BASED TEXT GENERATION\n",
        "# (SINGLE CELL – ASSIGNMENT READY)\n",
        "# =====================================\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Embedding, LayerNormalization, MultiHeadAttention\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# -------------------------\n",
        "# DATASET\n",
        "# -------------------------\n",
        "text = \"\"\"\n",
        "artificial intelligence is transforming modern society.\n",
        "it is used in healthcare finance education and transportation.\n",
        "machine learning allows systems to improve automatically with experience.\n",
        "data plays a critical role in training intelligent systems.\n",
        "large datasets help models learn complex patterns.\n",
        "deep learning uses multi layer neural networks.\n",
        "neural networks are inspired by biological neurons.\n",
        "each neuron processes input and produces an output.\n",
        "training a neural network requires optimization techniques.\n",
        "gradient descent minimizes the loss function.\n",
        "\n",
        "natural language processing helps computers understand human language.\n",
        "text generation is a key task in nlp.\n",
        "language models predict the next word or character.\n",
        "recurrent neural networks handle sequential data.\n",
        "lstm and gru models address long term dependency problems.\n",
        "however rnn based models are slow for long sequences.\n",
        "\n",
        "transformer models changed the field of nlp.\n",
        "they rely on self attention mechanisms.\n",
        "attention allows the model to focus on relevant context.\n",
        "transformers process data in parallel.\n",
        "this makes training faster and more efficient.\n",
        "modern language models are based on transformers.\n",
        "\n",
        "education is being improved using artificial intelligence.\n",
        "intelligent tutoring systems personalize learning.\n",
        "automated grading saves time for teachers.\n",
        "online education platforms use recommendation systems.\n",
        "technology enhances the quality of learning experiences.\n",
        "\n",
        "ethical considerations are important in artificial intelligence.\n",
        "fairness transparency and accountability must be ensured.\n",
        "ai systems should be designed responsibly.\n",
        "data privacy and security are major concerns.\n",
        "researchers continue to improve ai safety.\n",
        "\n",
        "text generation models can create stories poems and articles.\n",
        "they are used in chatbots virtual assistants and content creation.\n",
        "generated text should be meaningful and coherent.\n",
        "evaluation of text generation is challenging.\n",
        "human judgement is often required.\n",
        "\n",
        "continuous learning is essential in the field of ai.\n",
        "research and innovation drive technological progress.\n",
        "students should build strong foundations in mathematics.\n",
        "programming skills are important for ai engineers.\n",
        "practical experimentation enhances understanding.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "text = text.lower().replace(\"\\n\", \" \")\n",
        "\n",
        "# -------------------------\n",
        "# TOKENIZATION\n",
        "# -------------------------\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_index = {c: i for i, c in enumerate(chars)}\n",
        "index_to_char = {i: c for c, i in char_to_index.items()}\n",
        "\n",
        "vocab_size = len(chars)\n",
        "seq_length = 40\n",
        "embed_dim = 64\n",
        "num_heads = 2\n",
        "ff_dim = 128\n",
        "\n",
        "# -------------------------\n",
        "# CREATE INPUT-OUTPUT SEQUENCES\n",
        "# -------------------------\n",
        "X, y = [], []\n",
        "\n",
        "for i in range(len(text) - seq_length):\n",
        "    X.append([char_to_index[c] for c in text[i:i+seq_length]])\n",
        "    y.append(char_to_index[text[i+seq_length]])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# -------------------------\n",
        "# POSITIONAL ENCODING\n",
        "# -------------------------\n",
        "def positional_encoding(length, depth):\n",
        "    positions = np.arange(length)[:, np.newaxis]\n",
        "    depths = np.arange(depth)[np.newaxis, :]\n",
        "    angle_rates = 1 / np.power(10000, (2 * (depths // 2)) / np.float32(depth))\n",
        "    angle_rads = positions * angle_rates\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    return tf.cast(angle_rads[np.newaxis, ...], tf.float32)\n",
        "\n",
        "# -------------------------\n",
        "# BUILD TRANSFORMER MODEL\n",
        "# -------------------------\n",
        "inputs = Input(shape=(seq_length,))\n",
        "x = Embedding(vocab_size, embed_dim)(inputs)\n",
        "x += positional_encoding(seq_length, embed_dim)\n",
        "\n",
        "attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x, x)\n",
        "x = LayerNormalization()(x + attention)\n",
        "\n",
        "ffn = Dense(ff_dim, activation='relu')(x)\n",
        "ffn = Dense(embed_dim)(ffn)\n",
        "x = LayerNormalization()(x + ffn)\n",
        "\n",
        "outputs = Dense(vocab_size, activation='softmax')(x[:, -1])\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy'\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# TRAIN MODEL\n",
        "# -------------------------\n",
        "model.fit(X, y, epochs=15, batch_size=64)\n",
        "\n",
        "# -------------------------\n",
        "# TEXT GENERATION (WITH PADDING FIX)\n",
        "# -------------------------\n",
        "def generate_text(seed_text, length=300):\n",
        "    output = seed_text\n",
        "\n",
        "    for _ in range(length):\n",
        "        seq = output[-seq_length:]\n",
        "\n",
        "        # pad sequence if shorter than seq_length\n",
        "        if len(seq) < seq_length:\n",
        "            seq = \" \" * (seq_length - len(seq)) + seq\n",
        "\n",
        "        seq = np.array([[char_to_index[c] for c in seq]])\n",
        "        prediction = model.predict(seq, verbose=0)\n",
        "        next_char = index_to_char[np.argmax(prediction)]\n",
        "        output += next_char\n",
        "\n",
        "    return output\n",
        "\n",
        "# -------------------------\n",
        "# GENERATE TEXT\n",
        "# -------------------------\n",
        "seed = \"artificial intelligence \"\n",
        "generated_text = generate_text(seed)\n",
        "\n",
        "print(\"Generated Text using Transformer:\\n\")\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Y3S8bSIMshg1",
        "outputId": "6029f221-13ab-454d-c7eb-b4244e511a97"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 3.0200\n",
            "Epoch 2/15\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8487\n",
            "Epoch 3/15\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.8297\n",
            "Epoch 4/15\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6528\n",
            "Epoch 5/15\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.5554\n",
            "Epoch 6/15\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.4838\n",
            "Epoch 7/15\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.4964\n",
            "Epoch 8/15\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.4232\n",
            "Epoch 9/15\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.4180\n",
            "Epoch 10/15\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.4226\n",
            "Epoch 11/15\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.4101\n",
            "Epoch 12/15\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.3350\n",
            "Epoch 13/15\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.3235\n",
            "Epoch 14/15\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.2746\n",
            "Epoch 15/15\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.3107\n",
            "Generated Text using Transformer:\n",
            "\n",
            "artificial intelligence te tere ane tere te tere te te an ion are tere tere an ion ion are tere tere te are ane are tere tere te ate are are tere tere te an tere are are tere tere te an te are tere te tere te ate are tere te tere te an ion are tere tere te an te are tere te tere te ate are tere te tere te an ion are tere t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limitations of Transformer Models\n",
        "\n",
        "- Transformer models require large amounts of data to perform well.\n",
        "- Training transformers is computationally expensive and memory-intensive.\n",
        "- They are complex to design and tune compared to RNN-based models.\n",
        "- Performance may be limited on very small datasets.\n",
        "- Simpler models may be more suitable for small-scale applications.\n"
      ],
      "metadata": {
        "id": "by8Uc6Qysogt"
      }
    }
  ]
}