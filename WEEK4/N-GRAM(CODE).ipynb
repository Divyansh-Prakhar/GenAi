{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## N-gram Language Model\n",
        "\n",
        "An N-gram language model is a statistical approach used in Natural Language Processing to predict the next word or character in a sequence based on the previous *(nâˆ’1)* tokens. It learns patterns by counting the frequency of token sequences in a given text corpus. N-gram models are simple, interpretable, and commonly used as baseline models for text generation, but they are limited by their fixed context size.\n"
      ],
      "metadata": {
        "id": "JWXe4Wh9d2Xi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cghGC-mjsrLK",
        "outputId": "4409ba17-dfbf-462a-f762-0722bb03e469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text using N-gram Model:\n",
            "\n",
            "isfoforechiond iniomope. monge n. ioloud d ndexprngeunen cl hoh poplevatourkiacintalpompllin inedimsel in cauten. arng fintimolarastys smairarsch enenaren mins ls ms eslls syerearecongrofogicowionincesexthoul  sin t ems. rororandland decemuan rohatindederemod salans. ion lanchext intonss.  us. ty iol.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "text = \"\"\"\n",
        "artificial intelligence is transforming modern society.\n",
        "it is used in healthcare finance education and transportation.\n",
        "machine learning allows systems to improve automatically with experience.\n",
        "data plays a critical role in training intelligent systems.\n",
        "large datasets help models learn complex patterns.\n",
        "deep learning uses multi layer neural networks.\n",
        "neural networks are inspired by biological neurons.\n",
        "each neuron processes input and produces an output.\n",
        "training a neural network requires optimization techniques.\n",
        "gradient descent minimizes the loss function.\n",
        "\n",
        "natural language processing helps computers understand human language.\n",
        "text generation is a key task in nlp.\n",
        "language models predict the next word or character.\n",
        "recurrent neural networks handle sequential data.\n",
        "lstm and gru models address long term dependency problems.\n",
        "however rnn based models are slow for long sequences.\n",
        "\n",
        "transformer models changed the field of nlp.\n",
        "they rely on self attention mechanisms.\n",
        "attention allows the model to focus on relevant context.\n",
        "transformers process data in parallel.\n",
        "this makes training faster and more efficient.\n",
        "modern language models are based on transformers.\n",
        "\n",
        "education is being improved using artificial intelligence.\n",
        "intelligent tutoring systems personalize learning.\n",
        "automated grading saves time for teachers.\n",
        "online education platforms use recommendation systems.\n",
        "technology enhances the quality of learning experiences.\n",
        "\n",
        "ethical considerations are important in artificial intelligence.\n",
        "fairness transparency and accountability must be ensured.\n",
        "ai systems should be designed responsibly.\n",
        "data privacy and security are major concerns.\n",
        "researchers continue to improve ai safety.\n",
        "\n",
        "text generation models can create stories poems and articles.\n",
        "they are used in chatbots virtual assistants and content creation.\n",
        "generated text should be meaningful and coherent.\n",
        "evaluation of text generation is challenging.\n",
        "human judgement is often required.\n",
        "\n",
        "continuous learning is essential in the field of ai.\n",
        "research and innovation drive technological progress.\n",
        "students should build strong foundations in mathematics.\n",
        "programming skills are important for ai engineers.\n",
        "practical experimentation enhances understanding.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "text = text.lower().replace(\"\\n\", \" \")\n",
        "\n",
        "# -------------------------\n",
        "# BUILD BIGRAM MODEL\n",
        "# -------------------------\n",
        "n = 2  # Bigram\n",
        "ngram_model = defaultdict(list)\n",
        "\n",
        "for i in range(len(text) - n + 1):\n",
        "    prefix = text[i:i+n-1]#extracts first n word/tokens\n",
        "    next_char = text[i+n-1]#extracts the next possible charcaters\n",
        "    ngram_model[prefix].append(next_char)\n",
        "\n",
        "# -------------------------\n",
        "# TEXT GENERATION FUNCTION\n",
        "# -------------------------\n",
        "def generate_text(seed, length=300):\n",
        "    output = seed\n",
        "    for _ in range(length):\n",
        "        prefix = output[-(n-1):]#extracts prefix\n",
        "        next_char = random.choice(ngram_model.get(prefix, [' ']))#randomly selects next possible characters\n",
        "        output += next_char\n",
        "    return output\n",
        "\n",
        "# -------------------------\n",
        "# GENERATED OUTPUT\n",
        "# -------------------------\n",
        "seed_text = \"is\"\n",
        "generated_text = generate_text(seed_text)\n",
        "\n",
        "print(\"Generated Text using N-gram Model:\\n\")\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limitations of N-gram Model\n",
        "\n",
        "- **Limited Context:**  \n",
        "  N-gram models consider only a fixed number of previous tokens, so they cannot capture long-term dependencies in text.\n",
        "\n",
        "- **No Semantic Understanding:**  \n",
        "  The model is purely statistical and does not understand the meaning of words or sentences.\n",
        "\n",
        "- **Data Sparsity:**  \n",
        "  For larger values of n, many n-grams do not appear in the training data, leading to poor predictions.\n",
        "\n",
        "- **Poor Text Coherence:**  \n",
        "  Generated text often lacks grammatical structure and becomes incoherent for longer sequences.\n",
        "\n",
        "- **Not Scalable:**  \n",
        "  N-gram models require large memory and perform poorly on complex, real-world NLP tasks.\n"
      ],
      "metadata": {
        "id": "oGmLaPOLihjm"
      }
    }
  ]
}